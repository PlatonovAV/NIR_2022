Index: classification_algorithms_for_v3_test_data.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import timeit\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom matplotlib import pyplot as plt\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\r\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\r\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\r\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\nMAX_FEATURES = None\r\nN_JOBS = -1\r\n\r\n\r\ndef recall_specificity_scoring(df_a, scaler, clf):\r\n    def confusion_matrix_scorer(clf, X, y):\r\n        y_pred = clf.predict(X)\r\n        cm = confusion_matrix(y, y_pred)\r\n        return {'tp': cm[0, 0], 'fn': cm[0, 1], 'fp': cm[1, 0], 'tn': cm[1, 1]}\r\n\r\n    cv_results = cross_validate(clf.fit(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL']),\r\n                                scaler.fit_transform(df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a[\"LABEL\"],\r\n                                scoring=confusion_matrix_scorer)\r\n    recal = 0\r\n    specificit = 0\r\n    for n in range(len(cv_results['test_tp'])):\r\n        recal += cv_results['test_tp'][n] / (cv_results['test_tp'][n] + cv_results['test_fn'][n])\r\n        specificit += cv_results['test_tn'][n] / (cv_results['test_tn'][n] + cv_results['test_fp'][n])\r\n        print('полнота', cv_results['test_tp'][n] / (cv_results['test_tp'][n] + cv_results['test_fn'][n]))\r\n        print('специфичность', cv_results['test_tn'][n] / (cv_results['test_tn'][n] + cv_results['test_fp'][n]))\r\n        print()\r\n    recal = recal / len(cv_results['test_tp'])\r\n    specificit = specificit / len(cv_results['test_tp'])\r\n    print(\"******\\n\", \"Полнота: \", recal, '\\n', 'Специфичность: ', specificit, '\\n*****\\n')\r\n    print(cv_results)\r\n\r\n\r\ndef recall_specificity_scoring_no_scaler(df_a, clf):\r\n    def confusion_matrix_scorer(clf, X, y):\r\n        y_pred = clf.predict(X)\r\n        cm = confusion_matrix(y, y_pred)\r\n        return {'tp': cm[0, 0], 'fn': cm[0, 1], 'fp': cm[1, 0], 'tn': cm[1, 1]}\r\n\r\n    cv_results = cross_validate(clf.fit(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL']),\r\n                                (df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a[\"LABEL\"],\r\n                                scoring=confusion_matrix_scorer)\r\n    recal = 0\r\n    specificit = 0\r\n    for n in range(len(cv_results['test_tp'])):\r\n        recal += cv_results['test_tp'][n] / (cv_results['test_tp'][n] + cv_results['test_fn'][n])\r\n        specificit += cv_results['test_tn'][n] / (cv_results['test_tn'][n] + cv_results['test_fp'][n])\r\n        print('полнота', cv_results['test_tp'][n] / (cv_results['test_tp'][n] + cv_results['test_fn'][n]))\r\n        print('специфичность', cv_results['test_tn'][n] / (cv_results['test_tn'][n] + cv_results['test_fp'][n]))\r\n        print()\r\n    recal = recal / len(cv_results['test_tp'])\r\n    specificit = specificit / len(cv_results['test_tp'])\r\n    print(\"******\\n\", 'Полнота: ', recal, '\\n', 'Срецифичность: ', specificit, '\\n*****\\n')\r\n    print(cv_results)\r\n\r\n\r\ndef load_data(txt):\r\n    work_path = Path.cwd()\r\n    df_a_path = Path(work_path, 'dataset\\\\clear\\\\step3', txt)\r\n    df_a = pd.read_csv(df_a_path, sep=',', index_col=0)\r\n    df_a.index.names = [\"ID\"]\r\n    return df_a\r\n\r\n\r\ndef k_neighbors(txt):\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n\r\n    s = timeit.default_timer()\r\n    print(\"Классификация без использования скалирования\")\r\n    classifier = KNeighborsClassifier(n_jobs=N_JOBS)\r\n    classifier.fit(X_train, y_train)\r\n    y_pred = classifier.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n    e = timeit.default_timer()\r\n    print(\"elapsed time:\", e - s)\r\n\r\n    print()\r\n\r\n    s = timeit.default_timer()\r\n    print(\"Классификация с использованием скалирования\")\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n    classifier = KNeighborsClassifier(n_jobs=N_JOBS)\r\n    classifier.fit(X_train_scaler, y_train)\r\n    y_pred = classifier.predict(X_test_scaler)\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n    e = timeit.default_timer()\r\n    print(\"elapsed time:\", e - s)\r\n    # from sklearn.metrics import plot_confusion_matrix\r\n    # plot_confusion_matrix(classifier, X_test_scaler, y_test)\r\n    # plt.show()\r\n    print()\r\n\r\n    clf = KNeighborsClassifier(n_neighbors=300, algorithm='kd_tree', n_jobs=-1)\r\n    recall_specificity_scoring(df_a, scaler, clf)\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    knn = KNeighborsClassifier(algorithm=\"ball_tree\")\r\n    a_scaler = RobustScaler()\r\n    k_range = [10, 20, 50, 100, 150, 200, 250, 300, 400]\r\n    k_alg = ['ball_tree', 'kd_tree', 'brute']\r\n    param_grid = dict(n_neighbors=k_range, algorithm=k_alg)\r\n    grid = GridSearchCV(knn, param_grid, cv=5, scoring='balanced_accuracy', verbose=4, return_train_score=True,\r\n                        n_jobs=N_JOBS)\r\n    grid_search = grid.fit(a_scaler.fit_transform(df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    for n in k_range:\r\n        classifier = KNeighborsClassifier(n_neighbors=n, algorithm=\"kd_tree\", n_jobs=N_JOBS)\r\n        classifier.fit(X_train_scaler, y_train)\r\n        y_pred = classifier.predict(X_test_scaler)\r\n\r\n        quality = confusion_matrix(y_test, y_pred)\r\n        print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n        print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n        print('\\n')\r\n\r\n        col = (np.random.random(), np.random.random(), np.random.random())\r\n        Roc_data = classifier.predict_proba(X_test.values)\r\n        fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n        plt.plot(fpr_roc, tpr_roc, label='n = %s' % n, color=col)\r\n        plt.plot((0.0, 1.0), (0.0, 1.0))\r\n        plt.xlabel('True Positive Rate')\r\n        plt.ylabel('False Positive Rate')\r\n        plt.legend()\r\n\r\n\r\ndef random_forest():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    s = timeit.default_timer()\r\n    print(\"Классификация без использования скалирования\")\r\n    forest = RandomForestClassifier(random_state=42, n_estimators=10, n_jobs=N_JOBS)\r\n    forest.fit(X_train, y_train)\r\n    y_pred = forest.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n    e = timeit.default_timer()\r\n    print(\"elapsed time:\", e - s)\r\n\r\n    print()\r\n    s = timeit.default_timer()\r\n    print(\"Классификация с использованием скалирования\")\r\n    forest = RandomForestClassifier(random_state=42, n_estimators=10, n_jobs=N_JOBS)\r\n    forest.fit(X_train_scaler, y_train)\r\n    y_pred = forest.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n    e = timeit.default_timer()\r\n    print(\"elapsed time:\", e - s)\r\n    print()\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    rnd_frst = RandomForestClassifier(random_state=42)\r\n    k_range = [2, 5, 10, 20, 50, 100]\r\n    param_grid = dict(n_estimators=k_range)\r\n    a_scaler = RobustScaler()\r\n    grid = GridSearchCV(rnd_frst, param_grid, cv=3, scoring='roc_auc', verbose=3, return_train_score=True,\r\n                        n_jobs=N_JOBS)\r\n    grid_search = grid.fit(a_scaler.fit_transform(df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    for n in k_range:\r\n        classifier = RandomForestClassifier(n_estimators=n, random_state=42, max_features=MAX_FEATURES, n_jobs=N_JOBS)\r\n        classifier.fit(X_train_scaler, y_train)\r\n        y_pred = classifier.predict(X_test_scaler)\r\n\r\n        quality = confusion_matrix(y_test, y_pred)\r\n        print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n        print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n        print('\\n')\r\n\r\n        col = (np.random.random(), np.random.random(), np.random.random())\r\n        Roc_data = classifier.predict_proba(X_test_scaler)\r\n        fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n        plt.plot(fpr_roc, tpr_roc, label='ближайших соседей = %s ' % n, color=col)\r\n        plt.plot((0.0, 1.0), (0.0, 1.0))\r\n        plt.xlabel('True Positive Rate')\r\n        plt.ylabel('False Positive Rate')\r\n        plt.legend()\r\n\r\n\r\ndef decision_tree():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = StandardScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    dtree = DecisionTreeClassifier(random_state=42, max_features=MAX_FEATURES)\r\n    dtree.fit(X_train, y_train)\r\n    y_pred = dtree.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Классификация с использованием скалирования\")\r\n\r\n    cn = ['Computer Science', 'Physics']\r\n    dtree = DecisionTreeClassifier(random_state=42, max_depth=5, max_features=MAX_FEATURES)\r\n    dtree.fit(X_train_scaler, y_train)\r\n    y_pred = dtree.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    a_scaler = RobustScaler()\r\n    clf = DecisionTreeClassifier(random_state=42, max_features=MAX_FEATURES)\r\n    recall_specificity_scoring(df_a, a_scaler, clf)\r\n\r\n    clf = DecisionTreeClassifier(random_state=42)\r\n\r\n    k_criterion = ['gini', 'entropy', 'log_loss']\r\n    k_max_depth = [1, 2, 5, 10, 12, 15, 20, 30, 50, 100, 150, 200, None]\r\n    k_min_samples_split = [2, 5, 10, 12, 15, 20, 30, 50, 100, 150, 200, 300, 500, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\r\n    k_min_samples_leaf = [1, 2, 5, 10, 12, 15, 20, 30, 50, 100, 150, 200, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5,\r\n                          2.0, 3.0]\r\n    param_grid = dict(criterion=k_criterion, max_depth=k_max_depth, min_samples_split=k_min_samples_split\r\n                      , min_samples_leaf=k_min_samples_leaf)\r\n    time_start = timeit.default_timer()\r\n    grid = GridSearchCV(clf, param_grid, cv=5, scoring='roc_auc', verbose=1,\r\n                        return_train_score=True, n_jobs=N_JOBS)\r\n    grid_search = grid.fit(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n    time_stop = timeit.default_timer()\r\n    clf = DecisionTreeClassifier(criterion=grid_search.best_params_['criterion'],\r\n                                 max_depth=grid_search.best_params_['max_depth'],\r\n                                 min_samples_split=grid_search.best_params_['min_samples_split'],\r\n                                 min_samples_leaf=grid_search.best_params_['min_samples_leaf'])\r\n\r\n    recall_specificity_scoring(df_a, scaler, clf)\r\n    print(\"Время выполнения: \", time_stop - time_start, \"\\nВремя в минутах: \", (time_stop - time_start) / 60)\r\n\r\n    # roc_auc {'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 12, 'min_samples_split': 200}\r\n    # Accuracy for our training dataset with tuning is : 93.52%\r\n\r\n    # Полнота:  0.9007862242240403\r\n    # Специфичность:  0.8349609375\r\n\r\n    # balanced {'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 5, 'min_samples_split': 50}\r\n    # 87 %\r\n    # Полнота:  0.8755312477658282\r\n    #  Специфичность:  0.8681640625\r\n\r\n\r\ndef naive_bayes_bernoulli():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    nb = BernoulliNB()\r\n    nb.fit(X_train, y_train)\r\n    y_pred = nb.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Классификация с использованием скалирования\")\r\n\r\n    nb = BernoulliNB()\r\n    nb.fit(X_train_scaler, y_train)\r\n    y_pred = nb.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    clf = BernoulliNB()\r\n    recall_specificity_scoring(df_a, scaler, clf)\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    nb = BernoulliNB()\r\n    k_range = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 1.2, 1.5, 2.0, 2.2, 2.5, 3.0]\r\n    kk_range = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 1.2, 1.5, 2.0, 2.2, 2.5]\r\n    param_grid = dict(alpha=k_range, binarize=kk_range)\r\n\r\n    grid = GridSearchCV(nb, param_grid, cv=5, scoring='roc_auc', verbose=4, return_train_score=True, n_jobs=N_JOBS)\r\n    grid_search = grid.fit(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    clf = BernoulliNB(alpha=grid_search.best_params_['alpha'], binarize=grid_search.best_params_['binarize'])\r\n    recall_specificity_scoring(df_a, scaler, clf)\r\n\r\n\r\ndef naive_bayes_multinomial():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    nb = MultinomialNB()\r\n    nb.fit(X_train, y_train)\r\n    y_pred = nb.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    clf = MultinomialNB()\r\n    recall_specificity_scoring_no_scaler(df_a, clf)\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    nb = MultinomialNB()\r\n    k_range = list(np.arange(0.005, 3.0, 0.005))\r\n    param_grid = dict(alpha=k_range)\r\n    grid = GridSearchCV(nb, param_grid, cv=5, scoring='roc_auc', verbose=4, return_train_score=True, n_jobs=N_JOBS)\r\n    grid_search = grid.fit(X_train, y_train)\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    clf = MultinomialNB(alpha=grid_search.best_params_['alpha'])\r\n    recall_specificity_scoring_no_scaler(df_a, clf)\r\n\r\n\r\ndef nb_compare():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    print()\r\n    nbb = BernoulliNB()\r\n    nbm = MultinomialNB()\r\n\r\n    print(\"Метод Бернулли\")\r\n    nbb.fit(X_train, y_train)\r\n    y_pred = nbb.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    col = (np.random.random(), np.random.random(), np.random.random())\r\n    Roc_data = nbb.predict_proba(X_test)\r\n    fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n    plt.plot(fpr_roc, tpr_roc, label='Наивный Байес Бернулли', color=\"green\")\r\n    plt.plot((0.0, 1.0), (0.0, 1.0))\r\n    plt.xlabel('True Positive Rate')\r\n    plt.ylabel('False Positive Rate')\r\n    plt.legend()\r\n\r\n    print(\"Метод Гаусса\")\r\n    nbm.fit(X_train, y_train)\r\n    y_pred = nbm.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n    col = (np.random.random(), np.random.random(), np.random.random())\r\n    Roc_data = nbm.predict_proba(X_test)\r\n    fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n    plt.plot(fpr_roc, tpr_roc, label='Полиномиальный Наивный Байкс', color=\"red\")\r\n    plt.plot((0.0, 1.0), (0.0, 1.0))\r\n    plt.xlabel('True Positive Rate')\r\n    plt.ylabel('False Positive Rate')\r\n    plt.legend()\r\n\r\n\r\ndef bagging():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    base = KNeighborsClassifier()\r\n    base_classifier = base\r\n\r\n    print()\r\n    start_time = timeit.default_timer()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    clf = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42, n_jobs=N_JOBS)\r\n    clf.fit(X_train, y_train)\r\n    y_pred = clf.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Классификация с использованием скалирования\")\r\n    base_classifier = base\r\n    clf = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42, n_jobs=N_JOBS)\r\n    clf.fit(X_train_scaler, y_train)\r\n    y_pred = clf.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n    end_time = timeit.default_timer()\r\n    print('время выполнения: ', end_time - start_time)\r\n    print()\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    base_classifier = base\r\n    clf = BaggingClassifier(base_estimator=base_classifier, random_state=42, n_jobs=N_JOBS)\r\n    k_range = [2, 5, 10, 20, 50]\r\n    k_max_samples = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\r\n    param_grid = dict(n_estimators=k_range, max_samples=k_max_samples)\r\n    grid = GridSearchCV(clf, param_grid, scoring='balanced_accuracy', verbose=3, return_train_score=True, n_jobs=2)\r\n    grid_search = grid.fit(X_train_scaler, y_train)\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    a_scaler = RobustScaler()\r\n    clf = BaggingClassifier(base_estimator=base_classifier, random_state=42,\r\n                            n_estimators=grid_search.best_params_['n_estimators'],\r\n                            max_samples=grid_search.best_params_['max_samples']\r\n                            , n_jobs=N_JOBS)\r\n    recall_specificity_scoring(df_a, a_scaler, clf)\r\n\r\n    # метод случайных соседей, max_samples должно быть больше чем n_neighbors\r\n    # Оптимальные параметры:\r\n    # max_samples: 1.0 (0.2)\r\n    # n_estimators: 10 (50)\r\n    # 87 % (95)\r\n    # Оценка:\r\n    # Полнота:0.87 (0.65)\r\n    # Специфичность:0.88 (0.95)\r\n    # *********************\r\n    # метод случайного леса без параметров()\r\n    # Оптимальные параметры:\r\n    # 'max_samples': 1.0, 'n_estimators': 20 ()\r\n    # Оценка 88 % (94)\r\n    # Полнота:0.89 (0.89)\r\n    # Специфичность:0.89 (0.87)\r\n    # метод случайного леса c параметрами определёнными в первый раз\r\n    # Оптимальные параметры:\r\n    # {'max_samples': 1.0, 'n_estimators': 10} ({'max_samples': 1.0, 'n_estimators': 10})\r\n    # Оценка 80 (88)\r\n    # Полнота: 0.81 (0.82)\r\n    # Специфичность: 0.81 (0.80)\r\n    # *********************\r\n    # Полиномиальный Байес\r\n    # Оптимальные параметры: {'max_samples': 0.02, 'n_estimators': 50} ({'max_samples': 0.02, 'n_estimators': 50})\r\n    # Оценка ()\r\n    # Полнота:0.92 (0.92)\r\n    # Специфичность:0.88 (0.88)\r\n    # *********************\r\n    # Байес Бернулли\r\n    # Оптимальные параметры:\r\n    # {'max_samples': 1.0, 'n_estimators': 10} ({'max_samples': 0.5, 'n_estimators': 20})\r\n    # Оценка: 97\r\n    # Полнота: 0.91 0.91\r\n    # Специфичность: 0.91 0.91\r\n    # *********************\r\n\r\n    k_range = [1, 2, 5, 10, 20, 50]\r\n\r\n    base = KNeighborsClassifier(algorithm='kd_tree', n_neighbors=150)\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    base_classifier = base\r\n    for n in k_range:\r\n        clf = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, max_samples=1.0, random_state=42,\r\n                                n_jobs=N_JOBS)\r\n        clf.fit(X_train_scaler, y_train)\r\n        y_pred = clf.predict(X_test_scaler)\r\n\r\n        quality = confusion_matrix(y_test, y_pred)\r\n        print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n        print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n        col = (np.random.random(), np.random.random(), np.random.random())\r\n        Roc_data = clf.predict_proba(X_test_scaler)\r\n        fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n        plt.plot(fpr_roc, tpr_roc, label='Второй вариант оптимальных значений', color=col)\r\n        plt.plot((0.0, 1.0), (0.0, 1.0))\r\n        plt.xlabel('True Positive Rate')\r\n        plt.ylabel('False Positive Rate')\r\n        plt.legend()\r\n        print(\"n = \", n)\r\n        print('\\n')\r\n\r\n\r\ndef ada_boost():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = RobustScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    base_classifier = MultinomialNB()\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    clf = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\r\n    clf.fit(X_train, y_train)\r\n    y_pred = clf.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Классификация с использованием скалирования\")\r\n\r\n    clf = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\r\n    clf.fit(X_train_scaler, y_train)\r\n    y_pred = clf.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    clf = AdaBoostClassifier( n_estimators=10, random_state=42)\r\n\r\n    recall_specificity_scoring_no_scaler(df_a, clf)\r\n\r\n    print()\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    clf = AdaBoostClassifier(base_estimator=base_classifier,random_state=42)\r\n    k_range = [1,2,3,4,5, 10, 20, 30, 40, 50, 75, 100]\r\n    k_learning_rate = [0.01,0.02,0.03,0.04, 0.05, 0.1, 0.2, 0.3,0.4 ,0.5, 0.75, 1.0, 1.2, 1.5, 2.0]\r\n    k_algorithm = ['SAMME', 'SAMME.R']\r\n    param_grid = dict(n_estimators=k_range, learning_rate=k_learning_rate, algorithm=k_algorithm)\r\n    grid = GridSearchCV(clf, param_grid, scoring='balanced_accuracy', verbose=3, return_train_score=True, n_jobs=-1)\r\n    grid_search = grid.fit(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    clf = AdaBoostClassifier(base_estimator=base_classifier, random_state=42, n_estimators=grid_search.best_params_['n_estimators'],\r\n                             learning_rate=grid_search.best_params_['learning_rate'],\r\n                             algorithm=grid_search.best_params_['algorithm'])\r\n\r\n\r\n    recall_specificity_scoring_no_scaler(df_a,clf)\r\n\r\n    # Результаты работы алогритмов\r\n    # (в скобках указаны значения полученные при оценке 'roc_auc')\r\n    # Деревья решений при максимальной глубине равной 1\r\n    # Параметры\r\n    # {'algorithm': 'SAMME.R', 'learning_rate': 0.5, 'n_estimators': 200} ({'algorithm': 'SAMME.R', 'learning_rate': 0.5, 'n_estimators': 200})\r\n    # Оценка\r\n    # Проценты: (97,5) 0,92\r\n    # Полнота: (0.93) 0,93\r\n    # Специфичность: (0.91) 0,91\r\n    # ***************\r\n    # Деревья решений с определенными ранее параметрами\r\n    # Параметры\r\n    #  {'algorithm': 'SAMME.R', 'learning_rate': 0.5, 'n_estimators': 40} ({'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 200})\r\n    # Оценка\r\n    # Проценты: (97)\r\n    # Полнота: 0.92 (0.93)\r\n    # Специфичность: 0.91 (0.92)\r\n    # ***************\r\n    # Полиномиальный Наивный Байес\r\n    # Параметры\r\n    # {'algorithm': 'SAMME.R', 'learning_rate': 0.2, 'n_estimators': 100} ({'algorithm': 'SAMME.R', 'learning_rate': 0.4, 'n_estimators': 50})\r\n    # Оценка для roc_auc аналогично\r\n    # Проценты: 92\r\n    # Полнота: 0.91 (0,94)\r\n    # Специфичность: 0.93 (0,83)\r\n    # ***************\r\n    # Наивный Байес Бернулли\r\n    # Параметры\r\n    #{'algorithm': 'SAMME', 'learning_rate': 0.05, 'n_estimators': 100} ({'algorithm': 'SAMME', 'learning_rate': 0.05, 'n_estimators': 100})\r\n    # Оценка\r\n    # Проценты:\r\n    # Полнота: 0,94 (94)\r\n    # Специфичность:0,88 (88)\r\n\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    for n in k_range:\r\n        clf = AdaBoostClassifier(base_estimator=base_classifier,n_estimators=n, random_state=42,\r\n                                 algorithm= 'SAMME.R',\r\n                                 learning_rate=0.2)\r\n        clf = base_classifier\r\n        clf.fit(X_train, y_train)\r\n        y_pred = clf.predict(X_test)\r\n\r\n        quality = confusion_matrix(y_test, y_pred)\r\n        print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n        print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n        print('\\n')\r\n\r\n        col = (np.random.random(), np.random.random(), np.random.random())\r\n        Roc_data = clf.predict_proba(X_test)\r\n        fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n        plt.plot(fpr_roc, tpr_roc, label='полиномиальный Байес Бернулли', color='green')\r\n        plt.plot((0.0, 1.0), (0.0, 1.0))\r\n        plt.xlabel('True Positive Rate')\r\n        plt.ylabel('False Positive Rate')\r\n        plt.legend()\r\n        plt.show()\r\n\r\n\r\ndef gradient_boost():\r\n    # загрузка данных\r\n    df_a = load_data('train_v1.csv')\r\n\r\n    # формирование тестовых выборок\r\n    X_train, X_test, y_train, y_test = train_test_split(df_a.iloc[:, list(range(2, len(df_a.columns)))], df_a['LABEL'],\r\n                                                        train_size=0.75, random_state=42)\r\n    scaler = StandardScaler()\r\n    scaler.fit(X_train)\r\n    X_train_scaler = scaler.transform(X_train)\r\n    X_test_scaler = scaler.transform(X_test)\r\n\r\n    print('количество объектов в тренировочной выборке:\\n', y_train.value_counts(), \"\\n\")\r\n    print('количество объектов в тестовой выборке: \\n', y_test.value_counts(), \"\\n\")\r\n\r\n    print(\"Процентное соотношение выборок:\")\r\n    print(y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n    print(y_train.value_counts()[1] / (y_train.value_counts()[0] + y_train.value_counts()[1]) * 100, ' %')\r\n\r\n    print()\r\n    # без скалирования\r\n    print(\"Классификация без использования скалирования\")\r\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=42, learning_rate=0.01, max_depth=4,\r\n                                     max_features=MAX_FEATURES)\r\n    clf.fit(X_train, y_train)\r\n    y_pred = clf.predict(X_test)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Классификация с использованием скалирования\")\r\n\r\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=42, learning_rate=0.01, max_depth=2,\r\n                                     max_features=MAX_FEATURES)\r\n    clf.fit(X_train_scaler, y_train)\r\n    y_pred = clf.predict(X_test_scaler)\r\n\r\n    quality = confusion_matrix(y_test, y_pred)\r\n    print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n    print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n\r\n    print()\r\n\r\n    print(\"Поиск оптимальных значений\")\r\n    clf = GradientBoostingClassifier(random_state=42)\r\n    k_range = [2, 5, 10, 25, 50, 75, 100]\r\n    k_learning_rate = [0.01,0.05, 0.1, 0.5, 1.0,1.5,2.0]\r\n    k_max_depth = [2, 4, 8, 10, 20]\r\n    k_loss = ['log_loss','exponential']\r\n    k_criterion = ['friedman_mse','squared_error','mse']\r\n    k_subsample= [0.1,0.2,0.5,0.75,1.0]\r\n\r\n    param_grid = dict(n_estimators=k_range, learning_rate=k_learning_rate, max_depth=k_max_depth,loss=k_loss,criterion=k_criterion,subsample=k_subsample)\r\n    grid = GridSearchCV(clf, param_grid, scoring='roc_auc',cv=4, verbose=3, return_train_score=True, n_jobs=-1)\r\n    grid_search = grid.fit(scaler.fit_transform(df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a['LABEL'])\r\n    print(grid_search)\r\n    print(grid_search.best_params_)\r\n    accuracy = grid_search.best_score_ * 100\r\n    print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\r\n\r\n    a_scaler = RobustScaler()\r\n    clf = GradientBoostingClassifier(random_state=42,\r\n                                     n_estimators=grid_search.best_params_['k_range'],\r\n                                     learning_rate=grid_search.best_params_['k_learning_rate'],\r\n                                     max_depth=grid_search.best_params_['k_max_depth'],\r\n                                     loss=grid_search.best_params_['k_loss'],\r\n                                     criterion=grid_search.best_params_['k_criterion'],\r\n                                     subsample=grid_search.best_params_['k_subsample'])\r\n    recall_specificity_scoring(df_a,a_scaler,clf)\r\n\r\n    plt.figure(figsize=(10, 10)).clf()\r\n    for n in [5, 10, 50, 100]:\r\n        for m in [0.1, 0.5, 1.0]:\r\n            for k in [2, 4, 6]:\r\n                clf = GradientBoostingClassifier(n_estimators=n, random_state=42, learning_rate=m, max_depth=k,\r\n                                                 max_features=MAX_FEATURES)\r\n\r\n                clf.fit(X_train_scaler, y_train)\r\n                y_pred = clf.predict(X_test_scaler)\r\n                quality = confusion_matrix(y_test, y_pred)\r\n                print('параметры:\\nn_estimators=', n, '\\nlearning_rate=', m, '\\nmax_depth=', k)\r\n                print('полнота', quality[0, 0] / sum(quality[0, :]))\r\n                print('специфичность', quality[1, 1] / sum(quality[1, :]))\r\n                print('\\n')\r\n\r\n                col = (np.random.random(), np.random.random(), np.random.random())\r\n                Roc_data = clf.predict_proba(X_test_scaler)\r\n                fpr_roc, tpr_roc, threshold_roc = roc_curve(y_test, Roc_data[:, 1], pos_label='Physics')\r\n                plt.plot(fpr_roc, tpr_roc, label='n= {},m= {}, k={}'.format(n, m, k), color=col)\r\n                plt.plot((0.0, 1.0), (0.0, 1.0))\r\n                plt.xlabel('True Positive Rate')\r\n                plt.ylabel('False Positive Rate')\r\n                plt.legend()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/classification_algorithms_for_v3_test_data.py b/classification_algorithms_for_v3_test_data.py
--- a/classification_algorithms_for_v3_test_data.py	(revision 25eb069c06815ae417f6ce364efd3ef45df57ba3)
+++ b/classification_algorithms_for_v3_test_data.py	(date 1654014129631)
@@ -818,7 +818,7 @@
     k_subsample= [0.1,0.2,0.5,0.75,1.0]
 
     param_grid = dict(n_estimators=k_range, learning_rate=k_learning_rate, max_depth=k_max_depth,loss=k_loss,criterion=k_criterion,subsample=k_subsample)
-    grid = GridSearchCV(clf, param_grid, scoring='roc_auc',cv=4, verbose=3, return_train_score=True, n_jobs=-1)
+    grid = GridSearchCV(clf, param_grid, scoring='balanced_accuracy',cv=4, verbose=3, return_train_score=True, n_jobs=-1)
     grid_search = grid.fit(scaler.fit_transform(df_a.iloc[:, list(range(2, len(df_a.columns)))]), df_a['LABEL'])
     print(grid_search)
     print(grid_search.best_params_)
